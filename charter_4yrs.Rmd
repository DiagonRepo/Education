---
title: "Charter 2013"
author: "rjs8"
date: '07/30/2018'
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

## Indroduction
## Dataset Description

The dataset tracks the school's name, district, type (charter or not), student demographics, subject end of year test score averages, ranking in the state, class sizes, average parental education levels, and teacher credentials.

With the test scores as the response, we hope to model how charter schools will perform as compared to public schools using the other fields as predictors.

TODO: combine text 
## The Data Source

The file used for this analysis is at https://www.cde.ca.gov/ta/ac/ap/apidatafiles.asp, specifically, the "2013 Growth API".

## Background

School performance as recorded by the CA Dept. of Education from 1999-2013 (https://www.cde.ca.gov/ta/ac/ap/apidatafiles.asp)

## Interest

Interest in this dataset is driven by the current administration's Education Secretary's push to increase the number of charter schools.

## Methods

We will use API scores as our response. Since we are observing 2004, 2007, 2010, and 2013, we will use the API scores from these respective years as our response. For each year we will fit a model to the API score.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(foreign)
library(ggplot2)
library(latex2exp)
library(lmtest)    # provides bptest()
library(knitr)     # provides kable()
library(readr)
library(gridExtra)
library(car)       # provides car
library(caret)
```

```{r eval=FALSE,include=FALSE}
install.packages("foreign")
```

```{r}
set.seed(42)
data_2013 = read.dbf("data/api13gdb.dbf")
data_2010 = read.dbf("data/api10gdb.dbf", as.is = TRUE)
data_2007 = read.dbf("data/api07gdb.dbf", as.is = TRUE)
data_2004 = read.dbf("data/api04gdb.dbf", as.is = TRUE)
```

```{r eval=FALSE,include=FALSE}
View(data_2013)
```


## Response

The response variable for this analysis is going to be `API13`.

## Predictors

We start out by looking at the data and removing any rows that have missing data since we would be unable to provide estimates for their values.

```{r}
#
# Utility functions to manipulate the data.
#

# Function to remove exact column names
remove_columns = function(dframe, columns_to_remove) {
  subset(dframe, select = !(colnames(dframe) %in% columns_to_remove))
}

# Function to remove matching column names
remove_columns_like = function(dframe, columns_to_remove) {
  tmp_df = data.frame(dframe)
  for(match in columns_to_remove) {
    tmp_df = subset(tmp_df, select = -grep(match, colnames(tmp_df), perl=TRUE, value=FALSE))
  }
  tmp_df
}

# threshold is a number from 0 to 1 to indicate how much NA you want to see
# threshold = 0.7 means show columns with 70% NAs
show_columns_with_na = function(dframe, threshold = 0.5) {
  cn = colnames(dframe)
  col_na_ratios = rep(0, length(cn))
  for(c in 1:length(cn)) {
    col_na_ratios[c] = mean(is.na(dframe[,cn[c]]))
  }
  print(cn[col_na_ratios > threshold])
  cn[col_na_ratios > threshold]
}

# Remove columns common in dataset
remove_common_columns = function(dframe) {
  
  # Remove all _TARG, _MET, _SIG, columns which are just metadata about whether targets were met
  # or whether columns were significant.
  dframe = remove_columns_like(dframe, c("TARGET", "_TARG", "_MET", "_SIG", "_NUM", "_GROW"))

  # Remove rows with YR_RND = Yes because they have mostly NA columns
  dframe = subset(dframe, subset = dframe$YR_RND != 'Yes', select =!(colnames(dframe) %in% c('YR_RND')))

  # Remove the names because we have the school id in the first column
  dframe = remove_columns(dframe, c("SNAME", "DNAME", "CNAME"))

  # Remove rows with AVG_ED = NA because there's not that much of them (37)
  dframe = subset(dframe, subset = !is.na(dframe$AVG_ED))

  # Remove ACS_K3, ACS_46, ACS_CORE, FULL, EMER because it's mostly if not all is NA
  dframe = remove_columns(dframe, c("ACS_K3", "ACS_46", "ACS_CORE", "FULL", "EMER"))

  dframe
}

# Remove columns that are only there in 3 instead of 4 datasets
remove_less_common = function(dframe) {
  dframe$CHARTER = ifelse(is.na(dframe$CHARTER), "NC", dframe$CHARTER)

  # Transform SPED column by setting NA to 'R' for regular b/c it uses NA to indicate regular schools
  dframe$SPED = as.factor(ifelse(is.na(dframe$SPED), "R", dframe$SPED))
  
  # Remove schools with SIZE = S or T which indicate non-valid API scores
  dframe = subset(dframe, subset = !(dframe$SIZE %in% c("S","T")), select =!(colnames(dframe) %in% c('SIZE')))

  # Transform STYPE column by setting NA to 'O' to indicate other
  dframe$STYPE = as.factor(ifelse(is.na(dframe$STYPE), "O", dframe$STYPE))
}

# convert all columns listed in fac_pred to factors
conv_factor = function(dframe, columns_to_factor) {
  for(col in columns_to_factor) {
    if(col %in% colnames(dframe[0,])) {
      dframe[, col] = as.factor(dframe[, col])
    }
  }
  dframe
}

# convert all columsn to numeric
conv_numeric = function(dframe, columns_to_numeric) {
  for(col in columns_to_numeric) {
    if(col %in% colnames(dframe[0,])) {
      dframe[, col] = as.numeric(dframe[, col])
    }
  }
  dframe
}

# Function to calculate loocv_rmse
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

# Function to plot qqplot and perform shapiro and bptest
diagnostics = function(model, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, 
                       plotit = TRUE, testit = TRUE) {
  
  if (plotit == TRUE) {
    
    # side-by-side plots (one row, two columns)
    par(mfrow = c(1, 2))
    
    # fitted versus residuals
    plot(fitted(model), resid(model), 
         col = pcol, pch = 20, cex = 1.5, 
         xlab = "Fitted", ylab = "Residuals", 
         main = "Fitted versus Residuals")
    abline(h = 0, col = lcol, lwd = 2)
    grid()
    
    # qq-plot
    qqnorm(resid(model), col = pcol, pch = 20, cex = 1.5)
    qqline(resid(model), col = lcol, lwd = 2)
    grid()
  }
  
  if (testit == TRUE) {
    # p-value and decision
    shapiro_pval = shapiro.test(resid(model))$p.value
    bptest_pval = bptest((model))$p.value
    list(shapiro_pval = shapiro_pval, bptest_pval = bptest_pval)
  }

}
```


```{r}
str(data_2004)
```
```{r}
###################
# Common Columns
###################

# Define some common columns
# Factor Columns
fact_columns = c('RTYPE', 'STYPE', 'SPED', 'SIZE', 'CHARTER', "YR_RND",
                 "SCH_WIDE" , "COMP_IMP ", "BOTH", "AWARDS")

# Numeric Columns
num_columns = c(
             "API10", "API13", "API07", "API04",
             "API09", 
             'MEDIAN13','MEDIAN12','MEDIAN10','MEDIAN09','MEDIAN07','MEDIAN06','MEDIAN04','MEDIAN03',
             "ST_RANK", "SIM_RANK", 'sim_rank','st_rank', "SCI",
             "CDS",'VALID',
             "AA_API", "AI_API", "AS_API", "FI_API", "HI_API", "PI_API", "WH_API", "MR_API",
             "SD_API", "EL_API", "DI_API",
             "PCT_AA", "PCT_AI", "PCT_AS", "PCT_FI", "PCT_HI","PCT_PI", "PCT_WH", "PCT_MR",
             "MEALS", "P_GATE",  "P_MIGED", "P_EL", "P_RFEP", "P_DI", "CBMOB", "DMOB", 
             "ACS_K3", "ACS_46", "ACS_CORE", 
             "PCT_RESP", "NOT_HSG", "HSG", "SOME_COL", "COL_GRAD", "GRAD_SCH", "AVG_ED",
             "PEN_2", "PEN_35", "PEN_6", "PEN_78", "PEN_911","PEN_91",
             "ENROLL", "TESTED", 
             'VCST_E28','PCST_E28','VCST_E91','PCST_E91','CW_CSTE',
             'VCST_M28','PCST_M28','VCST_M91','PCST_M91','CW_CSTM',
             'VCST_S28','PCST_S28','VCST_S91','PCST_S91','CWS_91',
             'VCST_H28','PCST_H28','VCST_H91','PCST_H91','CW_CSTH',
             'VCSTM2_91','PCSTM2_91','CWM2_91','CWS2_91','VCSTS2_91','PCSTS2_91',
             'VCST_LS10','PCST_LS10','CWM2_28','VCSTM2_28','PCSTM2_28',
              'VCHS_E','PCHS_E','CW_CHSE',
             'VCHS_M','PCHS_M','CW_CHSM',
              'TOT_28','TOT_91','CW_SCI',
             "CAPA", "PAR_OPT", "VALID", "GROWTH","API09","TARGET","PAR_OPT","MEDIAN09"
             )

```

```{r}
#############################
# Data Cleaning for 2004
#############################
# remove comoonly removable columns
data_2004 = remove_common_columns(data_2004)

# Remove columns unique to the dataset
# _api is related to API which we are trying to measure
data_2004 = remove_columns_like(data_2004, c("_API04", "_API03"))

# Remove rows with MEDIAN13 or MEDIAN12 = NA
data_2004 = subset(data_2004, subset = !is.na(data_2004$MEDIAN04) | !is.na(data_2004$MEDIAN03))

# Iterate with any columns that have NAs: either transfrom the NA to something else, 
# or remove the column, or remove the row.
# Remove columns if they don't look like they're needed according to this dataset's record layout
# (https://www.cde.ca.gov/ta/ac/ap/reclayout13g.asp)
other_cols = show_columns_with_na(data_2004)
data_2004 = remove_columns(data_2004, other_cols)
data_2004 = na.omit(data_2004)

str(data_2004)
  
# convert columns to factor
data_2004 = conv_factor(data_2004, fact_columns)

# convert columns to numeric
data_2004 = conv_numeric(data_2004, num_columns)
nrow(data_2004)
ncol(data_2004)

```


```{r}
#############################
# Data Cleaning for 2007
#############################
# remove comoonly removable columns
data_2007 = remove_common_columns(data_2007)

data_2007$CHARTER = ifelse(is.na(data_2007$CHARTER), "NC", data_2007$CHARTER)

# Remove columns unique to the dataset
# _api is related to API which we are trying to measure
data_2007 = remove_columns_like(data_2007, c("_API07", "_API06"))

# Remove schools designated as small for GROWTH and BASE purposes. It uses NA to mean not-small.
data_2007 = subset(data_2007, subset = is.na(data_2007$sm07) & is.na(data_2007$sm06), select = !(colnames(data_2007) %in% c('sm07','sm06')))

# Remove rows with MEDIAN13 or MEDIAN12 = NA
data_2007 = subset(data_2007, subset = !is.na(data_2007$MEDIAN07) | !is.na(data_2007$MEDIAN06))

# Iterate with any columns that have NAs: either transfrom the NA to something else, 
# or remove the column, or remove the row.
# Remove columns if they don't look like they're needed according to this dataset's record layout
# (https://www.cde.ca.gov/ta/ac/ap/reclayout13g.asp)
other_cols = show_columns_with_na(data_2007)
data_2007 = remove_columns(data_2007, other_cols)
data_2007 = na.omit(data_2007)

# convert columns to factor
data_2007 = conv_factor(data_2007, fact_columns)

# convert columns to numeric
data_2007 = conv_numeric(data_2007, num_columns)
nrow(data_2007)
ncol(data_2007)

#############################
# Data Cleaning for 2010
#############################
# remove comoonly removable columns
data_2010 = remove_common_columns(data_2010)

data_2010$CHARTER = ifelse(is.na(data_2010$CHARTER), "NC", data_2010$CHARTER)

# Remove columns unique to the dataset
# _api is related to API which we are trying to measure
data_2010 = remove_columns_like(data_2010, c("_api10", "_API09"))

# Remove schools designated as small for GROWTH and BASE purposes. It uses NA to mean not-small.
data_2010 = subset(data_2010, subset = is.na(data_2010$sm10) & is.na(data_2010$sm09), select = !(colnames(data_2010) %in% c('sm10','sm09')))

# Remove rows with MEDIAN13 or MEDIAN12 = NA
data_2010 = subset(data_2010, subset = !is.na(data_2010$MEDIAN10) | !is.na(data_2010$MEDIAN09))

# Remove all fields with component test scores because this is what we're trying to find relationships for.
# TODO can move to common remove column
data_2010 = remove_columns_like(data_2010, c("VCST_", "PCST_", "VCHS_", "PCHS", "CW_", "CWM2_", "CWS2_", "VCSTM2_", "PCSTM2_", "CWS_", "VCSTS2_", "PCSTS2_", "TOT_"))

# also related to API
data_2010 = remove_columns(data_2010, c("API09", "TARGET", "GROWTH", "COMP_IMP", "PAR_OPT", "MEDIAN10", "MEDIAN09", "VALID"))
# Remove columns with too many NA
other_cols = show_columns_with_na(data_2010)
data_2010 = remove_columns(data_2010, other_cols)
data_2010 = na.omit(data_2010)

# convert columns to factor
data_2010 = conv_factor(data_2010, fact_columns)

# convert columns to numeric
data_2010 = conv_numeric(data_2010, num_columns)

str(data_2010)
nrow(data_2010)

#############################
# Data Cleaning for 2013
#############################
# remove comoonly removable columns
data_2013 = remove_common_columns(data_2013)

data_2013$CHARTER = ifelse(is.na(data_2013$CHARTER), "NC", data_2013$CHARTER)

# Remove columns unique to the dataset
# _api is related to API which we are trying to measure
data_2013 = remove_columns_like(data_2013, c("_API12", "_API13"))

# Remove schools designated as small for GROWTH and BASE purposes. It uses NA to mean not-small.
data_2013 = subset(data_2013, subset = is.na(data_2013$SM12) & is.na(data_2013$SM13), select = !(colnames(data_2013) %in% c('SM12','SM13')))

# Remove rows with MEDIAN13 or MEDIAN12 = NA
data_2013 = subset(data_2013, subset = !is.na(data_2013$MEDIAN13) | !is.na(data_2013$MEDIAN12))

# Iterate with any columns that have NAs: either transfrom the NA to something else, 
# or remove the column, or remove the row.
# Remove columns if they don't look like they're needed according to this dataset's record layout 
# (https://www.cde.ca.gov/ta/ac/ap/reclayout13g.asp)
other_cols = show_columns_with_na(data_2013)
data_2013 = remove_columns(data_2013, other_cols)
data_2013 = na.omit(data_2013)

# convert columns to factor
data_2013 = conv_factor(data_2013, fact_columns)

# convert columns to numeric
data_2013 = conv_numeric(data_2013, num_columns)
nrow(data_2013)
```

```{r}
str(data_2010)
```

```{r}
# group 13
set.seed(13)
#
# Randomly draw 600 samples from dataset
# Split samples into train and test
#

# 2004
# Split the dataset into training and test data set
# Randomly select 600 observations from 2010 data set
ridx = sample(nrow(data_2004), 600)
data_2004 = data_2004[ridx, ]

# split dataframe into training and test
(n = nrow(data_2004))
trn_idx = sample(nrow(data_2004), n/2)
data_2004_trn = data_2004[trn_idx, ]
data_2004_tst = data_2004[-trn_idx, ]

# 2007
# Split the dataset into training and test data set
# Randomly select 600 observations from 2010 data set
ridx = sample(nrow(data_2007), 600)
data_2007 = data_2007[ridx, ]

# split dataframe into training and test
(n = nrow(data_2007))
trn_idx = sample(nrow(data_2007), n/2)
data_2007_trn = data_2007[trn_idx, ]
data_2007_tst = data_2007[-trn_idx, ]

# 2010
# Split the dataset into training and test data set
# Randomly select 600 observations from 2010 data set
ridx = sample(nrow(data_2010), 600)
data_2010 = data_2010[ridx, ]

# split dataframe into training and test
(n = nrow(data_2010))
trn_idx = sample(nrow(data_2010), n/2)
data_2010_trn = data_2010[trn_idx, ]
data_2010_tst = data_2010[-trn_idx, ]

# 2013
# Split the dataset into training and test data set
# Randomly select 600 observations from 2010 data set
ridx = sample(nrow(data_2013), 600)
data_2013 = data_2013[ridx, ]

# split dataframe into training and test
(n = nrow(data_2013))
trn_idx = sample(nrow(data_2013), n/2)
data_2013_trn = data_2013[trn_idx, ]
data_2013_tst = data_2013[-trn_idx, ]
```

Next, we perform variable selection on an additive model.
```{r}
# remove school identifier
data_2013 = remove_columns(data_2013, c("CDS"))

unique(data_2013$RTYPE) # yields only one factor in dataset, so remove the column
data_2013 = remove_columns(data_2013, c("RTYPE"))

# Remove all fields with component test scores because this is what we're trying to find relationships for.
data_2013 = remove_columns(data_2013, c('TESTED','MED13','MED12','VCST_E28','PCST_E28','VCST_E91','PCST_E91','VCST_M28','PCST_M28','VCST_M91',
'PCST_M91','VCST_S28','PCST_S28','VCST_S91','PCST_S91','VCST_H28','PCST_H28','VCST_H91','PCST_H91','VCHS_E',
'PCHS_E','VCHS_M','PCHS_M','TOT_28','TOT_91','MEDIAN13','MEDIAN12','chg_data','VCST_LS10','PCST_LS10',
'VCSTM2_28','PCSTM2_28','173VCSTM2_91','174PCSTM2_91','VCSTS2_91','PCSTS2_91','IRG5','sim_rank','st_rank'))
data_2013 = remove_columns(data_2013, c("VCSTM2_91","PCSTM2_91"))
```

```{r}
# model_initial = lm(API13 ~ ., data=data_2013)
# model_sel = step(model_initial, trace = 0)
# summary(model_sel)
```


### Tests for Normality

We test the model for normality to make sure our model is valid.

QQPlots
```{r}
# qqnorm(resid(model_sel))
# qqline(resid(model_sel))
# plot(resid(model_sel), fitted(model_sel))
```



BPTest and Shapiro Test
```{r}
# bptest(model_sel)
# shapiro.test(resid(model_sel)[1:3000])
```

The tests for normality fail. We must now see if we can reduce the model to one that is normally distributed.


### Normality Adjustments
```{r}
# summary(model_sel)
```

The last three columns look suspicious. 
```{r eval=FALSE, include=FALSE}
unique(data_2013$CWM2_28)
unique(data_2013$CWM2_91)
unique(data_2013$CWS2_91)
sum(data_2013$CWM2_28==1)
sum(data_2013$CWM2_28==2)
sum(data_2013$CWM2_28==3)
sum(data_2013$CWM2_28==4)

# Remove these columns from the model and re-fit
# model_init = lm(formula = API13 ~ STYPE + CHARTER + VALID + PCT_AA + PCT_AI + 
#     PCT_AS + PCT_FI + PCT_HI + PCT_PI + PCT_WH + PCT_MR + MEALS + 
#     P_MIGED + P_DI + CBMOB + DMOB + ACS_K3 + ACS_46 + ACS_CORE + 
#     NOT_HSG + HSG + SOME_COL + GRAD_SCH + AVG_ED + FULL + EMER + 
#     PEN_2 + PEN_35 + PEN_6 + PEN_78 + PEN_911 + ENROLL + PAR_OPT + 
#     CW_CSTE + CWS_91 + CW_CSTH + CW_CHSM + CW_SCI, data = data_2013)

model_init = lm(formula = API13 ~ STYPE + CHARTER + VALID + PCT_AA + PCT_AI + 
    PCT_AS + PCT_FI + PCT_HI + PCT_PI + PCT_WH + PCT_MR + MEALS + 
    P_MIGED + P_DI + CBMOB + DMOB + 
    NOT_HSG + HSG + SOME_COL + GRAD_SCH + AVG_ED + 
    PEN_2 + PEN_35 + PEN_6 + PEN_78 + PEN_911 + ENROLL + PAR_OPT + 
    CW_CSTE + CWS_91 + CW_CSTH + CW_CHSM + CW_SCI, data = data_2013)

model = step(model_init, trace=0)
summary(model)
bptest(model)
shapiro.test(resid(model)[1:3000])
qqnorm(resid(model))
qqline(resid(model))
plot(resid(model), fitted(model))
```

Next we look at the `pairs` plot to see if any of the predictors are collinear.

```{r}
pairs(subset(data_2013,select=c('API13', 'VALID', 'PCT_AA', 'PCT_AI')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'PCT_AS', 'PCT_FI', 'PCT_HI')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'PCT_PI', 'PCT_WH', 'PCT_MR')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'MEALS', 'P_MIGED', 'P_DI')))
```

```{r eval=FALSE, include=FALSE}
pairs(subset(data_2013,select=c('API13', 'CBMOB', 'DMOB', 'ACS_K3')))
```

```{r eval=FALSE, include=FALSE}
pairs(subset(data_2013,select=c('API13', 'ACS_46', 'ACS_CORE', 'NOT_HSG')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'HSG', 'SOME_COL', 'GRAD_SCH')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'AVG_ED', 'FULL', 'EMER')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'PEN_2', 'PEN_35', 'PEN_6')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'PEN_78', 'PEN_911', 'ENROLL')))
```

```{r eval=FALSE, include=FALSE}
pairs(subset(data_2013,select=c('API13', 'PAR_OPT', 'CW_CSTE', 'CWS_91')))
```

```{r eval=FALSE, include=FALSE}
pairs(subset(data_2013,select=c('API13', 'CW_CSTH', 'CW_CHSM', 'CW_SCI')))
```

It looks like the following fields may be collinear: DMOB with CBMOB, PEN_2 with PEN_35, PEN_6, PEN_78, and PEN_911. The collinear columns will be removed.

The following fields look like they can use a `log()` transform: PCT_AA, PCT_AS, P_MIGED, P_DI, CBMOB, and FULL. 

### Striated Sub-Sampling

```{r eval=FALSE, include=FALSE}
# Apply backwards AIC to the given model and plot the QQPlot and residual vs fitted
model_it = function(a_model) {
  opt_model = step(a_model, trace=0)
  qqnorm(resid(opt_model))
  qqline(resid(opt_model))
  plot(resid(opt_model), fitted(opt_model))
  opt_model
}

```

```{r eval=FALSE, include=FALSE}
# Data still not passing normality tests.
# Try striated partitions: 30 random samples for range 11-598
randomly_select = function(dframe, no_to_select) {
  dframe[sample(nrow(dframe), no_to_select), ] # it's up to the caller to make sure there are enough rows
}

striation_subsample = function(dframe, response, no_bins = 20, bin_size = 30) {
  range_response = range(dframe[, response])
  response_interval = (range_response[2] - range_response[1])/no_bins
  no_breaks = seq(range_response[1], range_response[2], by=response_interval)
  
  reduced_data = data.frame(dframe[0,]) # First add the column headers only
  for(i in seq(range_response[1], range_response[2], by=response_interval)) {
    interval_rows = dframe[, response] >= i & dframe[, response] < (i + response_interval)

    no_rows_in_interval = nrow(dframe[interval_rows,])

    print(paste(i,' => ',no_rows_in_interval,' elts'))
    if (no_rows_in_interval > bin_size) {
      reduced_data = rbind(reduced_data, 
                           randomly_select(subset(dframe, subset = interval_rows),
                                           bin_size))
    } else { # add the entire bin to the resulting dataset
      reduced_data = rbind(reduced_data,
                           subset(dframe, subset = interval_rows))
    }
  }
  print(paste('Reduced model has',nrow(reduced_data),'rows'))
  reduced_data
}

hist(data_2013$API13)
reduced_data = striation_subsample(data_2013, 'API13', 20, 30)
hist(reduced_data$API13)
```


```{r eval=FALSE, include=FALSE}
# Test reduced model; STYPE and VALID showed high collinearity in a VIF output so those were removed
model_init = lm(API13 ~ CHARTER + log(PCT_AA) + PCT_AI + 
                  log(PCT_AS) + PCT_FI + PCT_HI + PCT_PI + PCT_WH + PCT_MR + MEALS + 
                  log(P_MIGED) + log(P_DI) + log(CBMOB) + ACS_K3 + ACS_46 + ACS_CORE + 
                  NOT_HSG + HSG + SOME_COL + GRAD_SCH + AVG_ED + log(FULL) + EMER + 
                  ENROLL + PAR_OPT + 
                  CWS_91 + CW_CSTH + CW_CHSM + CW_SCI, data = reduced_data)
print("Model Optimization 1")
model_opt1 = model_it(model_init)
summary(model_opt1)
bptest(model_opt1)
shapiro.test(resid(model_opt1))

print("After cooks distance applied")
cd = cooks.distance(model_opt1)
no_outlier_data = subset(reduced_data, subset = cd < 3/length(cd)) # using an aggressive cooks distance criteria
print(paste(nrow(no_outlier_data),"rows left"))

model_init2 = lm(API13 ~ CHARTER + log(PCT_AA) + PCT_AI + 
                   log(PCT_AS) + PCT_FI + PCT_HI + PCT_PI + PCT_WH + PCT_MR + MEALS + 
                   log(P_MIGED) + log(P_DI) + log(CBMOB) + ACS_K3 + ACS_46 + ACS_CORE + 
                   NOT_HSG + HSG + SOME_COL + GRAD_SCH + AVG_ED + log(FULL) + EMER + 
                   ENROLL + PAR_OPT + 
                   CWS_91 + CW_CSTH + CW_CHSM + CW_SCI, data = no_outlier_data)
print("Model Optimization 2")
model_opt2 = model_it(model_init2)
summary(model_opt2)
bptest(model_opt2)
shapiro.test(resid(model_opt2))

```
```{r eval=FALSE, include=FALSE}
# check collinearity
car::vif(model_opt2)
```


```{r}
#################
# 2004
#################
#   
# mod_2004 = lm(API04  ~ PCT_AS +  log(PCT_HI)  + PCT_WH   +   DMOB + PCT_RESP +  (AVG_ED) +  ENROLL , data = data_2004_trn )
# 
# 
# # check assumptions
# diagnostics(mod_2004)
# 
# # check how the model is fitting
# summary(mod_2004)
# vif(mod_2004) 
# calc_loocv_rmse(mod_2004)
# 
# pred = predict(mod_2004, newdata = data_2004_tst)
# plot(pred ~ mod_2004$API04, col = "dodgerblue")

#################
# 2007
#################
# schl2010_fit = lm(API10  ~ PEN_2  +  PEN_35 + PEN_6   + PEN_78  + poly(PCT_AA, 2) + poly(PCT_AI, 1) + poly(PCT_AS, 2) + poly(PCT_WH, 2) + poly(P_GATE, 1) + P_MIGED + poly(AVG_ED, 4) + poly(MEALS, 1) ,
#                data = data_2010_trn)
  
#  mod_2007 = lm(API07  ~ PEN_2 +  PEN_35 +  PCT_AS +  log(PCT_HI)  + PCT_WH     + P_GATE +    P_EL  +   DMOB + PCT_RESP +  poly(AVG_ED, 3) +  ENROLL , data = data_2007_trn )
#   
#   
# # check assumptions
# diagnostics(mod_2007)
# 
# # check how the model is fitting
# summary(mod_2007)
# vif(mod_2007) 
# calc_loocv_rmse(mod_2007)
# 
# pred = predict(mod_2007, newdata = data_2007_tst)
# plot(pred ~ schl2010_tst$API07, col = "dodgerblue")

#################
# 2010
#################
# mod_2010 = lm(API10  ~ PEN_2  +  PEN_35 + PEN_6   + PEN_78  + poly(PCT_AA, 2) + poly(PCT_AI, 1) + poly(PCT_AS, 2) + poly(PCT_WH, 2) + poly(P_GATE, 1) + P_MIGED + poly(AVG_ED, 4) + poly(MEALS, 1) ,
#                data = data_2004_trn)
  
 mod_2010 = lm(API10  ~ PEN_2 +  PEN_35 +  poly(PCT_AS, 2) +  (PCT_HI)  + PCT_WH     + P_GATE +    P_EL  +   DMOB + PCT_RESP +  poly(AVG_ED, 3) +  ENROLL + MEALS, data = data_2010_trn )


# check assumptions
diagnostics(mod_2010)

# check how the model is fitting
summary(mod_2010)
vif(mod_2010)
calc_loocv_rmse(mod_2010)

pred = predict(mod_2010, newdata = data_2010_tst)
plot(pred ~ schl2010_tst$API10, col = "dodgerblue")

#################
# 2013
#################
# schl2013_fit = lm(API13  ~ PEN_78 + PEN_911 + poly(PCT_AA, 2) + poly(PCT_AI, 2) + poly(PCT_AS, 3) + poly(PCT_WH, 2) + poly(P_GATE, 1) + P_MIGED + poly(AVG_ED, 4) + MEALS ,
#                data = schl2013_trn)
 # schl2013_fit = lm(API13  ~ PEN_2 +  PEN_35 +  PCT_AS +  log(PCT_HI)  + (PCT_WH)     + P_GATE +    P_EL  +   DMOB + PCT_RESP +  poly(AVG_ED, 3) +  ENROLL , data = schl2013_trn )

mod_2013 = lm(API13 ~    PEN_2 +  PEN_35 +   PEN_6  + PEN_78 +  poly(PCT_AS, 2) +  poly(PCT_FI, 1) +   PCT_HI +   poly(PCT_WH, 1)  +   P_EL  +  log(CBMOB) + poly(AVG_ED, 4), data =  data_2013_trn)
  
# check assumptions
diagnostics(mod_2013)

# check how the model is fitting
summary(mod_2013)
vif(mod_2013) 
calc_loocv_rmse(mod_2013)

pred = predict(mod_2013, newdata = data_2013_tst)
plot(pred ~ schl2013_tst$API13, col = "dodgerblue")
```

## Conclusions 2013

Even though the bptest rejected the null hypothesis of a constant variance, the `Fitted vs Residual` plot doesn't show any discernible patterns. The Shapiro test failed to reject the null hypothesis that the data was sampled from a normal distribution. The `vif()` check doesn't show collinearity for any of the final set of predictors. The QQ-Plot looks good.

When this script is run repeatedly, there are two large influencers that emerge. One case looks like this:
```{r}
final1 = lm(API13 ~ log(PCT_AA) + PCT_FI + PCT_HI + PCT_PI + 
              log(P_MIGED) + log(CBMOB) + ACS_46 + ACS_CORE + HSG + AVG_ED + 
              EMER + CWS_91 + CW_CSTH + CW_CHSM, data=no_outlier_data)
bptest(final1)
shapiro.test(resid(final1))
qqnorm(resid(final1))
qqline(resid(final1))
plot(resid(final1), fitted(final1))
summary(final1)$adj  
coef(final1)
```

The other case looks like this:
```{r}
final2 = lm(API13 ~ CHARTER + log(PCT_AA) + PCT_FI + PCT_HI + 
              PCT_PI + log(P_DI) + ACS_46 + ACS_CORE + GRAD_SCH + AVG_ED + 
              EMER + CWS_91 + CW_CHSM + CW_SCI, data=no_outlier_data)
bptest(final2)
shapiro.test(resid(final2))
qqnorm(resid(final2))
qqline(resid(final2))
plot(resid(final2), fitted(final2))
summary(final2)$adj 
coef(final2)
```


In the end, the largest influencers of performance are:

* CBMOB - whether the student doesn't skip school day for more than 30 consecutive days from when the school starts
* AVG_ED - the parent's education level
* ACS_CORE - the class size
* CHARTER - whether the school is a charter school or not
* PCT_AA and PCT_PI - the presence of some disadvantaged minority and ethnic groups

It's important to note that these influencers illustrate correlation and not causation.





