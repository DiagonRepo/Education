---
title: "Charter 2013"
author: "rjs8"
date: '07/30/2018'
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(foreign)
library(ggplot2)
library(latex2exp)
library(lmtest)    # provides bptest()
library(knitr)     # provides kable()
library(readr)
library(gridExtra)
library(car)       # provides car
library(caret)
```

```{r eval=FALSE,include=FALSE}
install.packages("foreign")
```

```{r}
data_2013 = read.dbf("data/api13gdb.dbf")
```

```{r eval=FALSE,include=FALSE}
View(data_2013)
```


## The Data Source

The file used for this analysis is at https://www.cde.ca.gov/ta/ac/ap/apidatafiles.asp, specifically, the "2013 Growth API".

## Response

The response variable for this analysis is going to be `API13`.

## Predictors

We start out by looking at the data and removing any rows that have missing data since we would be unable to provide estimates for their values.
```{r}
# Clean up rows with NAs
#data_2013=na.omit(data_2013) # apparently there are NAs in every row; this gave me an empty dataframe

# First get the subset that the documentation says has valid statistical data,
# then remove the FLAG column because we don't need it anymore.
# Also remove the other alternative responses related to growth and APIs.
data_2013 = subset(data_2013, subset = is.na(data_2013$FLAG), select = !(colnames(data_2013) %in% c('FLAG','API12','TARGET','GROWTH','SCH_WIDE','COMP_IMP','BOTH')))
nrow(data_2013)
```
```{r}
#
# Utility functions to manipulate the data.
#

# Function to remove exact column names
remove_columns = function(dframe, columns_to_remove) {
  subset(dframe, select = !(colnames(dframe) %in% columns_to_remove))
}

# Function to remove matching column names
remove_columns_like = function(dframe, columns_to_remove) {
  tmp_df = data.frame(dframe)
  for(match in columns_to_remove) {
    tmp_df = subset(tmp_df, select = -grep(match, colnames(tmp_df), perl=TRUE, value=FALSE))
  }
  tmp_df
}

# threshold is a number from 0 to 1 to indicate how much NA you want to see
# threshold = 0.7 means show columns with 70% NAs
show_columns_with_na = function(dframe, threshold) {
  cn = colnames(dframe)
  col_na_ratios = rep(0, length(cn))
  for(c in 1:length(cn)) {
    col_na_ratios[c] = mean(is.na(dframe[,cn[c]]))
  }
  print(cn[col_na_ratios > threshold])
  cn[col_na_ratios > threshold]
}
```

```{r remove NAs}
# Remove all _TARG, _MET, _SIG, _API12, _API13 columns which are just metadata about whether targets were met
# or whether columns were significant.
data_2013 = remove_columns_like(data_2013, c("_TARG", "_MET", "_SIG", "_API12", "_API13", "_NUM", "_GROW"))

# Remove schools designated as small for GROWTH and BASE purposes. It uses NA to mean not-small.
data_2013 = subset(data_2013, subset = is.na(data_2013$SM12) & is.na(data_2013$SM13), select = !(colnames(data_2013) %in% c('SM12','SM13')))

# Remove schools with SIZE = S or T which indicate non-valid API scores
data_2013 = subset(data_2013, subset = !(data_2013$SIZE %in% c("S","T")), select =!(colnames(data_2013) %in% c('SIZE')))

# Transform CHARTER column to indicate Y or N because it uses NA to indicate a non-charter school
data_2013$CHARTER = as.factor(ifelse(is.na(data_2013$CHARTER), "N", "Y"))


# Transform SPED column by setting NA to 'R' for regular b/c it uses NA to indicate regular schools
data_2013$SPED = as.factor(ifelse(is.na(data_2013$SPED), "R", data_2013$SPED))

# Transform STYPE column by setting NA to 'O' to indicate other
data_2013$STYPE = as.factor(ifelse(is.na(data_2013$STYPE), "O", data_2013$STYPE))

# Remove the names because we have the school id in the first column
data_2013 = remove_columns(data_2013, c("SNAME", "DNAME", "CNAME"))

# Remove rows with YR_RND = Yes because they have mostly NA columns
data_2013 = subset(data_2013, subset = data_2013$YR_RND != 'Yes', select =!(colnames(data_2013) %in% c('YR_RND')))

# Remove rows with AVG_ED = NA because there's not that much of them (37)
data_2013 = subset(data_2013, subset = !is.na(data_2013$AVG_ED))

# Remove rows with st_rank and sim_rank = NA b/c there's not that much of them (1 each)
data_2013 = subset(data_2013, subset = !is.na(data_2013$st_rank) & !is.na(data_2013$sim_rank))

# Remove rows with MEDIAN13 or MEDIAN12 = NA
data_2013 = subset(data_2013, subset = !is.na(data_2013$MEDIAN13) | !is.na(data_2013$MEDIAN12))

# Iterate with any columns that have NAs: either transfrom the NA to something else, 
# or remove the column, or remove the row.
# Remove columns if they don't look like they're needed according to this dataset's record layout 
# (https://www.cde.ca.gov/ta/ac/ap/reclayout13g.asp)

other_cols = show_columns_with_na(data_2013, 0)
data_2013 = remove_columns(data_2013, other_cols)

nrow(data_2013)
```


Next step is to convert numeric columns back to NUM and factor columns to FACTOR.
```{r}
num_columns = c('CDS','VALID','API13','PCT_AA','PCT_AI','PCT_AS','PCT_FI','PCT_HI','PCT_PI','PCT_WH',
'PCT_MR','MEALS','P_GATE','P_MIGED','P_EL','P_RFEP','P_DI','CBMOB','DMOB','ACS_K3',
'ACS_46','ACS_CORE','PCT_RESP','NOT_HSG','HSG','SOME_COL','COL_GRAD','GRAD_SCH','AVG_ED','FULL',
'EMER','PEN_2','PEN_35','PEN_6','PEN_78','PEN_911','ENROLL','PAR_OPT','TESTED','MEDIAN13',
'MEDIAN12','VCST_E28','PCST_E28','VCST_E91','PCST_E91','CW_CSTE','VCST_M28','PCST_M28','VCST_M91','PCST_M91',
'CW_CSTM','VCST_S28','PCST_S28','VCST_S91','PCST_S91','CWS_91','VCST_H28','PCST_H28','VCST_H91','PCST_H91',
'CW_CSTH','VCHS_E','PCHS_E','CW_CHSE','VCHS_M','PCHS_M','CW_CHSM','TOT_28','TOT_91','CW_SCI',
'VCST_LS10','PCST_LS10','CWM2_28','VCSTM2_28','PCSTM2_28','CWM2_91','VCSTM2_91','PCSTM2_91','CWS2_91','VCSTS2_91',
'PCSTS2_91','sim_rank','st_rank')
for(c in 1:length(num_columns)) {
  data_2013[,num_columns[c]] = as.numeric(data_2013[,num_columns[c]])
}
```


Next, we perform variable selection on an additive model.
```{r}
# remove school identifier
data_2013 = remove_columns(data_2013, c("CDS"))

unique(data_2013$RTYPE) # yields only one factor in dataset, so remove the column
data_2013 = remove_columns(data_2013, c("RTYPE"))

# Remove all fields with component test scores because this is what we're trying to find relationships for.
data_2013 = remove_columns(data_2013, c('TESTED','MED13','MED12','VCST_E28','PCST_E28','VCST_E91','PCST_E91','VCST_M28','PCST_M28','VCST_M91',
'PCST_M91','VCST_S28','PCST_S28','VCST_S91','PCST_S91','VCST_H28','PCST_H28','VCST_H91','PCST_H91','VCHS_E',
'PCHS_E','VCHS_M','PCHS_M','TOT_28','TOT_91','MEDIAN13','MEDIAN12','chg_data','VCST_LS10','PCST_LS10',
'VCSTM2_28','PCSTM2_28','173VCSTM2_91','174PCSTM2_91','VCSTS2_91','PCSTS2_91','IRG5','sim_rank','st_rank'))
data_2013 = remove_columns(data_2013, c("VCSTM2_91","PCSTM2_91"))
```

```{r}
model_initial = lm(API13 ~ ., data=data_2013)
model_sel = step(model_initial, trace = 0)
summary(model_sel)
```


### Tests for Normality

We test the model for normality to make sure our model is valid.

QQPlots
```{r}
qqnorm(resid(model_sel))
qqline(resid(model_sel))
plot(resid(model_sel), fitted(model_sel))
```



BPTest and Shapiro Test
```{r}
bptest(model_sel)
shapiro.test(resid(model_sel)[1:3000])
```

The tests for normality fail. We must now see if we can reduce the model to one that is normally distributed.


### Normality Adjustments
```{r}
summary(model_sel)
```

The last three columns look suspicious. 
```{r}
unique(data_2013$CWM2_28)
unique(data_2013$CWM2_91)
unique(data_2013$CWS2_91)
sum(data_2013$CWM2_28==1)
sum(data_2013$CWM2_28==2)
sum(data_2013$CWM2_28==3)
sum(data_2013$CWM2_28==4)

# Remove these columns from the model and re-fit
model_init = lm(formula = API13 ~ STYPE + CHARTER + VALID + PCT_AA + PCT_AI + 
    PCT_AS + PCT_FI + PCT_HI + PCT_PI + PCT_WH + PCT_MR + MEALS + 
    P_MIGED + P_DI + CBMOB + DMOB + ACS_K3 + ACS_46 + ACS_CORE + 
    NOT_HSG + HSG + SOME_COL + GRAD_SCH + AVG_ED + FULL + EMER + 
    PEN_2 + PEN_35 + PEN_6 + PEN_78 + PEN_911 + ENROLL + PAR_OPT + 
    CW_CSTE + CWS_91 + CW_CSTH + CW_CHSM + CW_SCI, data = data_2013)
model = step(model_init, trace=0)
summary(model)
bptest(model)
shapiro.test(resid(model)[1:3000])
qqnorm(resid(model))
qqline(resid(model))
plot(resid(model), fitted(model))
```

Next we look at the `pairs` plot to see if any of the predictors are collinear.

```{r}
pairs(subset(data_2013,select=c('API13', 'VALID', 'PCT_AA', 'PCT_AI')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'PCT_AS', 'PCT_FI', 'PCT_HI')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'PCT_PI', 'PCT_WH', 'PCT_MR')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'MEALS', 'P_MIGED', 'P_DI')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'CBMOB', 'DMOB', 'ACS_K3')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'ACS_46', 'ACS_CORE', 'NOT_HSG')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'HSG', 'SOME_COL', 'GRAD_SCH')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'AVG_ED', 'FULL', 'EMER')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'PEN_2', 'PEN_35', 'PEN_6')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'PEN_78', 'PEN_911', 'ENROLL')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'PAR_OPT', 'CW_CSTE', 'CWS_91')))
```

```{r}
pairs(subset(data_2013,select=c('API13', 'CW_CSTH', 'CW_CHSM', 'CW_SCI')))
```

It looks like the following fields may be collinear: DMOB with CBMOB, PEN_2 with PEN_35, PEN_6, PEN_78, and PEN_911. The collinear columns will be removed.

The following fields look like they can use a `log()` transform: PCT_AA, PCT_AS, P_MIGED, P_DI, CBMOB, and FULL. 

### Striated Sub-Sampling

```{r}
# Apply backwards AIC to the given model and plot the QQPlot and residual vs fitted
model_it = function(a_model) {
  opt_model = step(a_model, trace=0)
  qqnorm(resid(opt_model))
  qqline(resid(opt_model))
  plot(resid(opt_model), fitted(opt_model))
  opt_model
}

```

```{r}
# Data still not passing normality tests.
# Try striated partitions: 30 random samples for range 11-598
randomly_select = function(dframe, no_to_select) {
  dframe[sample(nrow(dframe), no_to_select), ] # it's up to the caller to make sure there are enough rows
}

no_bins  = 20 # decide on how big to make the bins
bin_size = 30 # number of rows to have in each bin
range_response = range(data_2013$API13)
response_interval = (range_response[2] - range_response[1])/no_bins
no_breaks = seq(range_response[1], range_response[2], by=response_interval)

reduced_data = data.frame(data_2013[0,]) # First add the column headers only
for(i in seq(range_response[1], range_response[2], by=response_interval)) {
  no_rows_in_interval = nrow(data_2013[data_2013$API13 >= i & data_2013$API13 < (i + response_interval),])
  print(paste(i,' => ',no_rows_in_interval,' elts'))
  if (no_rows_in_interval > bin_size) {
    reduced_data = rbind(reduced_data, 
                         randomly_select(subset(data_2013, 
                                                subset = data_2013$API13 >= i & data_2013$API13 < (i + response_interval)),
                                         bin_size))
  } else { # add the entire bin to the resulting dataset
    reduced_data = rbind(reduced_data,
                         subset(data_2013, 
                                subset = data_2013$API13 >= i & data_2013$API13 < (i + response_interval)))
  }
}
hist(data_2013$API13,breaks = no_breaks)
hist(reduced_data$API13)
print(paste('Reduced model has',nrow(reduced_data),'rows'))
```


```{r}
# Test reduced model; STYPE and VALID showed high collinearity in a VIF output so those were removed
model_init = lm(API13 ~ CHARTER + log(PCT_AA) + PCT_AI + 
                  log(PCT_AS) + PCT_FI + PCT_HI + PCT_PI + PCT_WH + PCT_MR + MEALS + 
                  log(P_MIGED) + log(P_DI) + log(CBMOB) + ACS_K3 + ACS_46 + ACS_CORE + 
                  NOT_HSG + HSG + SOME_COL + GRAD_SCH + AVG_ED + log(FULL) + EMER + 
                  ENROLL + PAR_OPT + 
                  CWS_91 + CW_CSTH + CW_CHSM + CW_SCI, data = reduced_data)
print("Model Optimization 1")
model_opt1 = model_it(model_init)
summary(model_opt1)
bptest(model_opt1)
shapiro.test(resid(model_opt1))

print("After cooks distance applied")
cd = cooks.distance(model_opt1)
no_outlier_data = subset(reduced_data, subset = cd < 3/length(cd)) # using an aggressive cooks distance criteria
print(paste(nrow(no_outlier_data),"rows left"))

model_init2 = lm(API13 ~ CHARTER + log(PCT_AA) + PCT_AI + 
                   log(PCT_AS) + PCT_FI + PCT_HI + PCT_PI + PCT_WH + PCT_MR + MEALS + 
                   log(P_MIGED) + log(P_DI) + log(CBMOB) + ACS_K3 + ACS_46 + ACS_CORE + 
                   NOT_HSG + HSG + SOME_COL + GRAD_SCH + AVG_ED + log(FULL) + EMER + 
                   ENROLL + PAR_OPT + 
                   CWS_91 + CW_CSTH + CW_CHSM + CW_SCI, data = no_outlier_data)
print("Model Optimization 2")
model_opt2 = model_it(model_init2)
summary(model_opt2)
bptest(model_opt2)
shapiro.test(resid(model_opt2))

```
```{r}
# check collinearity
car::vif(model_opt2)
```

## Conclusions 2013

Even though the bptest rejected the null hypothesis of a constant variance, the `Fitted vs Residual` plot doesn't show any discernible patterns. The Shapiro test failed to reject the null hypothesis that the data was sampled from a normal distribution. The `vif()` check doesn't show collinearity for any of the final set of predictors. The QQ-Plot looks good.

When this script is run repeatedly, there are two large influencers that emerge. One case looks like this:
```{r}
final1 = lm(API13 ~ log(PCT_AA) + PCT_FI + PCT_HI + PCT_PI + 
              log(P_MIGED) + log(CBMOB) + ACS_46 + ACS_CORE + HSG + AVG_ED + 
              EMER + CWS_91 + CW_CSTH + CW_CHSM, data=no_outlier_data)
bptest(final1)
shapiro.test(resid(final1))
qqnorm(resid(final1))
qqline(resid(final1))
plot(resid(final1), fitted(final1))
summary(final1)$adj  
coef(final1)
```

The other case looks like this:
```{r}
final2 = lm(API13 ~ CHARTER + log(PCT_AA) + PCT_FI + PCT_HI + 
              PCT_PI + log(P_DI) + ACS_46 + ACS_CORE + GRAD_SCH + AVG_ED + 
              EMER + CWS_91 + CW_CHSM + CW_SCI, data=no_outlier_data)
bptest(final2)
shapiro.test(resid(final2))
qqnorm(resid(final2))
qqline(resid(final2))
plot(resid(final2), fitted(final2))
summary(final2)$adj 
coef(final2)
```


In the end:

* positive influence on the overall performance for 2013 seems to be whether the student doesn't skip school day for more than 30 consecutive days from when the school starts, and the parent's education level
* negative influencers include whether the school is a charter school or not, and the presence of some disadvantaged minority and ethnic groups

It's important to note that these influencers illustrate correlation and not causation.






